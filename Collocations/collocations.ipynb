{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Домашнее задание №1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "OpqB6E0VDFWX"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "import math\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Предобработка\n",
        "file = (open('Москва-Петушки.txt', 'r', encoding = 'utf-8')).read()\n",
        "stop_words = stopwords.words(\"russian\")"
      ],
      "metadata": {
        "id": "3F5XjIa6pd4Z"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Знаки препинания\n",
        "punct = re.compile(\"[^\\w\\s]\")\n",
        "file_without_punct = re.sub(punct, \"\", file)"
      ],
      "metadata": {
        "id": "kv4FVm-cpN8H"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Регистр\n",
        "file_without_punct = file_without_punct.lower()"
      ],
      "metadata": {
        "id": "-SSNA6uUPUdS"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Токенизация\n",
        "info_tokenized = word_tokenize(file_without_punct)"
      ],
      "metadata": {
        "id": "Y8gmhKLE60ls"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Лемматизация\n",
        "info_lemmatized = []\n",
        "for token in info_tokenized:\n",
        "    lemma = morph.parse(token)[0]\n",
        "    lemma = lemma.normal_form\n",
        "    info_lemmatized.append(lemma)"
      ],
      "metadata": {
        "id": "mr8iQ8jj-zYY"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Фильтрация от стоп-слов\n",
        "info_filtered = []\n",
        "for lemma in info_lemmatized:\n",
        "    if lemma not in stop_words:\n",
        "        info_filtered.append(lemma)"
      ],
      "metadata": {
        "id": "mRNSmk4FwHKB"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Частотный словарь\n",
        "dictionary = {}\n",
        "for token in info_filtered:\n",
        "    if token in dictionary:\n",
        "        dictionary[token] += 1\n",
        "    else:\n",
        "        dictionary[token] = 1"
      ],
      "metadata": {
        "id": "n3z-C50GqL_Y"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Сортируем частотный словарь\n",
        "sorted_dictionary = sorted(dictionary.items(), key=lambda x:x[1], reverse = True)\n",
        "length = len(info_filtered)"
      ],
      "metadata": {
        "id": "R726_ZI_s2vD"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Создаём частотный список\n",
        "sorted_list=[]\n",
        "for word in sorted_dictionary:\n",
        "    frequency_list = list(word)\n",
        "    sorted_list.append(frequency_list)"
      ],
      "metadata": {
        "id": "n5PIviAnDkn0"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 1\n",
        "for i in sorted_list:\n",
        "    logfr = math.log(int(i[1]), 10)\n",
        "    logrank = math.log(k, 10)  # берём десятичные логарифмы для ранга и для частоты\n",
        "    i.append(k)\n",
        "    i.append(logfr)\n",
        "    i.append(logrank)\n",
        "    k += 1"
      ],
      "metadata": {
        "id": "yYtWaqO3DpMY"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Экспорт статистики в файл\n",
        "frame = pd.DataFrame(sorted_list, columns=['Word', 'Frequency', 'Rank', 'Logarithm of Frequency', 'Logarithm of rank'])\n",
        "frame.to_csv('Frequency Dictionary.csv',encoding='windows-1251', sep=';', index=False)"
      ],
      "metadata": {
        "id": "g2kBEnGrDxwi"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Биграммы и триграммы\n",
        "bigrams = nltk.collocations.BigramAssocMeasures()\n",
        "trigrams = nltk.collocations.TrigramAssocMeasures()\n",
        "\n",
        "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(info_filtered)\n",
        "trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(info_filtered)\n",
        "\n",
        "# bigrams\n",
        "bigram_freq = bigramFinder.ngram_fd.items()\n",
        "bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
        "bigramFreqTable.to_csv('Bigrams.csv',encoding='windows-1251', sep=';', index=False)\n",
        "\n",
        "# trigrams\n",
        "trigram_freq = trigramFinder.ngram_fd.items()\n",
        "trigramFreqTable = pd.DataFrame(list(trigram_freq), columns=['trigram','freq']).sort_values(by='freq', ascending=False)\n",
        "trigramFreqTable.to_csv('Trigrams.csv',encoding='windows-1251', sep=';', index=False)"
      ],
      "metadata": {
        "id": "_MP_VtLUD0QI"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# t-test\n",
        "bigramTtable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.student_t)), columns=['bigram','T-test']).sort_values(by='T-test', ascending=False)\n",
        "trigramTtable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.student_t)), columns=['trigram','T-test']).sort_values(by='T-test', ascending=False)\n",
        "bigramTtable = bigramTtable.head(100)\n",
        "trigramTtable = trigramTtable.head(100) #вычленяем первые 100 лучших би- и триграммов\n",
        "bigramTtable.to_csv('Bigrams-T-test-100.csv',encoding='windows-1251', sep=';', index=False) #экспортируем в файл\n",
        "trigramTtable.to_csv('Trigrams-T-test-100.csv',encoding='windows-1251', sep=';', index=False) #экспортируем в файл\n",
        "\n",
        "# chi-squared\n",
        "bigramCHItable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.chi_sq)), columns=['bigram','Chi-squared']).sort_values(by='Chi-squared', ascending=False)\n",
        "trigramCHItable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.chi_sq)), columns=['trigram','Chi-squared']).sort_values(by='Chi-squared', ascending=False)\n",
        "bigramCHItable = bigramCHItable.head(100)\n",
        "trigramCHItable = trigramCHItable.head(100) #вычленяем первые 100 лучших би- и триграммов\n",
        "bigramCHItable.to_csv('Bigrams-CHI-test-100.csv',encoding='windows-1251', sep=';', index=False) #экспортируем в файл\n",
        "trigramCHItable.to_csv('Trigrams-CHI-test-100.csv',encoding='windows-1251', sep=';', index=False) #экспортируем в файл\n",
        "\n",
        "# log-likelihood\n",
        "bigramlikelihoodtable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.likelihood_ratio)), columns=['bigram','Log-Likelihood']).sort_values(by='Log-Likelihood', ascending=False)\n",
        "trigramlikelihoodtable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.likelihood_ratio)), columns=['trigram','Log-Likelihood']).sort_values(by='Log-Likelihood', ascending=False)\n",
        "bigramlikelihoodtable = bigramlikelihoodtable.head(100)\n",
        "trigramlikelihoodtable = trigramlikelihoodtable.head(100) #вычленяем первые 100 лучших би- и триграммов\n",
        "bigramlikelihoodtable.to_csv('Bigrams-likelihood-test-100.csv',encoding='windows-1251', sep=';', index=False) #экспортируем в файл\n",
        "trigramlikelihoodtable.to_csv('Trigrams-likelihood-test-100.csv',encoding='windows-1251', sep=';', index=False) #экспортируем в файл\n",
        "\n",
        "# pmi\n",
        "bigramPMItable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.pmi)), columns=['bigram','PMI']).sort_values(by='PMI', ascending=False)\n",
        "trigramPMItable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.pmi)), columns=['trigram','PMI']).sort_values(by='PMI', ascending=False)\n",
        "bigramPMItable = bigramPMItable.head(100)\n",
        "trigramPMItable = trigramPMItable.head(100) #вычленяем первые 100 лучших би- и триграммов\n",
        "bigramPMItable.to_csv('Bigrams-PMI-test-100.csv',encoding='windows-1251', sep=';', index=False) #экспортируем в файл\n",
        "trigramPMItable.to_csv('Trigrams-PMI-test-100.csv',encoding='windows-1251', sep=';', index=False) #экспортируем в файл"
      ],
      "metadata": {
        "id": "1syIWe3ZD8Y8"
      },
      "execution_count": 61,
      "outputs": []
    }
  ]
}